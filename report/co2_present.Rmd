---
title: "Global $CO_{2}$ Emissions in 1997"
short: "What Keeling missed all these years"
author: "Meral Basit, Alex Hubbard, Mohamed Bakr"
output:
  bookdown::pdf_document2:
    toc: true
    # citation_package: natbib
    fig_caption: true
    latex_engine: pdflatex
fontsize: 11pt
geometry: margin=1in
editor_options: 
  chunk_output_type: inline
bibliography: BibFile.bib
csl: data-science-journal.csl
---

```{r setup, echo=FALSE}
## default to not show code, unless we ask for it.
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE, dpi=1000)
options(digits = 3)
```

```{r load packages, echo = FALSE, message = FALSE}
library(tidyverse)
library(tsibble)
```



<!-- # Report from the Point of View of the Present -->

<!-- One of the very interesting features of Keeling and colleagues' research is that they were able to evaluate, and re-evaluate the data as new series of measurements were released. This permitted the evaluation of previous models' performance and a much more difficult question: If their models' predictions were "off" was this the result of a failure of the model, or a change in the system? -->

# Background
<!-- ## (1 point) Task 0b: Introduction -->

<!-- In this introduction, you can assume that your reader will have **just** read your 1997 report. In this introduction, **very** briefly pose the question that you are evaluating, and describe what (if anything) has changed in the data generating process between 1997 and the present. -->

Since our last report, the volcano where the research center is located erupted. Therefore the measurements from Dec. 2022 to July 4, 2023 are from the Maunakea Observatories, which are just over 20 miles north of the original observatory. Additionally, there is a note that the last several months worth of data is "preliminary" and therefore could be revised.

** TO DO add question**

### Null Hypothesis


# Measurement and Data
<!-- The most current data is provided by the United States' National Oceanic and Atmospheric Administration, on a data page [[here](https://gml.noaa.gov/ccgg/trends/data.html)]. Gather the most recent weekly data from this page. (A group that is interested in even more data management might choose to work with the [hourly data](https://gml.noaa.gov/aftp/data/trace_gases/co2/in-situ/surface/mlo/co2_mlo_surface-insitu_1_ccgg_HourlyData.txt).) -->

<!-- Create a data pipeline that starts by reading from the appropriate URL, and ends by saving an object called `co2_present` that is a suitable time series object. -->

<!-- Conduct the same EDA on this data. Describe how the Keeling Curve evolved from 1997 to the present, noting where the series seems to be following similar trends to the series that you "evaluated in 1997" and where the series seems to be following different trends. This EDA can use the same, or very similar tools and views as you provided in your 1997 report. -->

```{r read data}
co2_present <- read_csv("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv", skip = 35)

co2_present <- co2_present %>% 
  mutate(
    date_time = make_datetime(year, month, day),
    year_week = yearweek(date_time)
  )

# remove values less than 0
co2_present <- co2_present %>% 
  filter(average > 0)

co2_present <- co2_present %>% 
  as_tsibble(index = year_week) %>% 
  fill_gaps()
```

```{r EDA standard plots for present data}
# time series
p8 <- ggplot(co2_present, aes(x = date_time, y = average)) +
  geom_line(linewidth = .5) +
  labs(title = "Atmospheric CO2 1974-2024", x = "Date", y = "CO2 (ppm)") +
  theme_minimal()

# acf
p9 <- ggAcf(co2_present$average) + ggtitle("ACF CO2")

# pacf
p10 <- ggPacf(co2_present$average) + ggtitle("PACF CO2")

# histogram
p11 <- ggplot(co2_present, aes(x = average)) +
  geom_histogram(bins = 40, fill = "lightblue", color = "black") +
  labs(title = "Histogram of Atmospheric CO2", x = "CO2 (ppm)", y = "Frequency")

# annual average
p12 <- as_tibble(co2_present) %>%
  mutate(year = year(date_time)) %>%
  group_by(year) %>%
  summarise(avg_value = mean(average, na.rm=TRUE)) %>%
  ggplot(aes(x=year, y=avg_value)) +
  geom_point() +
  labs(title = "Average Yearly CO2 Levels", x = "Year", y = "Average CO2 (ppm)") +
  theme_minimal()

# make layout
(p8 + p11) /
  (p9 + p10) /
  p12

```

We see that there is a strong linear trend in the CO2 levels, as seen by the bottom plot of average yearly CO2. It appears that this linear trend very slightly increased in slope after the year 2000. We also see that the distribution of values is fairly wide from the histogram, with a slight right skew. We also see that the ACF tails off very slowly while the PACF drops off after lag 1. This indicates that there may be some unit roots. As this timeseries is a continuation of our previous time series, we know that this data is also not stationary.

# Model Comparisons

## Linear Model
<!-- ## (1 point) Task 2b: Compare linear model forecasts against realized CO2 -->

<!-- Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from a linear time model in 1997 (i.e. "Task 2a"). (You do not need to run any formal tests for this task.) -->

```{r}
ggplot(co2_present, aes(x = year_week)) +
  geom_line(aes(y = average, colour = "Actual")) +
  geom_line(data = yearmonth_2020, aes(x = index, y = value, colour = "Predicted")) +
  labs(title = "CO2 Levels with Fitted Linear Trend", x = "Year", y = "CO2 (ppm)") +
  theme_minimal()
```

Our polynomial model looks like it did a pretty good job at predicting CO2 up to 2020. It is missing the peaks and valleys, but it looks to capture the average yearly increase in CO2 levels.

## ARIMA Model
<!-- ## (1 point) Task 3b: Compare ARIMA models forecasts against realized CO2 -->

<!-- Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from the ARIMA model that you fitted in 1997 (i.e. "Task 3a"). Describe how the Keeling Curve evolved from 1997 to the present. -->

```{r}
ggplot(co2_present, aes(x = year_week)) +
  geom_line(aes(y = average, colour = "Actual")) +
  geom_line(data = forecast_2022 %>% filter(.model == "arima_fit_log")
            , aes(x = index, y = .mean, colour = "Predicted - Log")) +
  geom_line(data = forecast_2022 %>% filter(.model == "arima_fit")
            , aes(x = index, y = .mean, colour = "Predicted")) +
  labs(title = "CO2 Levels with Fitted ARIMA Trend", x = "Year", y = "CO2 (ppm)") +
  theme_minimal()
```

<!-- ## (3 points) Task 4b: Evaluate the performance of 1997 linear and ARIMA models -->

<!-- In 1997 you made predictions about the first time that CO2 would cross 420 ppm. How close were your models to the truth? -->

```{r predictions of 420 ppm}
co2_present %>% 
  filter(average > 420)
```

In 1997, we predicted that CO2 levels would reach 420 PPM by March 2021, about a year early. Actual CO2 levels reached 420 PPM by March of 2022.

After reflecting on your performance on this threshold-prediction task, continue to use the weekly data to generate a month-average series from 1997 to the present, and compare the overall forecasting performance of your models from Parts 2a and 3b over the entire period. (You should conduct formal tests for this task.)

```{r month-average series from 1997 to present}
co2_present <- co2_present %>% 
  mutate(
    index = yearmonth(year_week)
  )

monthly_avg <- co2_present %>%
  index_by(index) %>%
  summarise(value = mean(average, na.rm = TRUE)) %>% 
  filter(index >= ymd(19950101))

forecast_present <- co2_fit %>% 
  forecast(monthly_avg)

forecast_present %>% 
  accuracy(monthly_avg)
```

Our ARIMA model with log transformation produces the lowest RMSE and MAE values compared with the ARIMA model and the random walk models.

<!-- ## (4 points) Task 5b: Train best models on present data -->

<!-- Seasonally adjust the weekly NOAA data, and split both seasonally-adjusted (SA) and non-seasonally-adjusted (NSA) series into training and test sets, using the last two years of observations as the test sets. For both SA and NSA series, fit ARIMA models using all appropriate steps. Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample, comparing candidate models and explaining your choice. In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model. -->

```{r split new data}
co2_present_SA_train <- co2_present %>%
  filter(date_time < as.Date("2022-06-30")) %>%
  mutate(average = difference(average,52))

co2_present_NSA_train <- co2_present %>%
  filter(date_time < as.Date("2022-06-30"))

co2_present_SA_test <- co2_present %>%
  filter(date_time >= as.Date("2022-06-30")) %>%
  mutate(average = difference(average,52))

co2_present_NSA_test <- co2_present %>%
  filter(date_time >= as.Date("2022-06-30"))

```

```{r new data stationary}
# SA Data
co2_present_SA_train %>% 
  features(average, unitroot_kpss)

co2_present_SA_train %>% 
  mutate(diff = difference(average)) %>% 
  features(diff, unitroot_kpss)

# NSA Data
co2_present_NSA_train %>% 
  features(average, unitroot_kpss)

co2_present_NSA_train %>% 
  mutate(diff = difference(average)) %>% 
  features(diff, unitroot_kpss)
  
```

Identically to before, we use the KPSS test to determine whether the SA and the NSA data are stationary. For both series, the first test yields a p value of 0.01, and we reject the null hypothesis, meaning that our data is not stationary. After taking one difference, we see that our p value for both series is 0.1, and we fail to reject the null hypothesis, meaning that the both datasets are stationary after one difference.

```{r make SA model}
# Seasonally-Adjusted
co2_fit_present_SA <- co2_present_SA_train %>% 
  fill_gaps() %>%
  model(
    random_walk = ARIMA(average ~ 0 + pdq(0,1,0) + PDQ(0,0,0)),
    arima_fit = ARIMA(average),
    arima_fit_log = ARIMA(log(average))
  )
report(co2_fit_present_SA[1])
report(co2_fit_present_SA[2])
report(co2_fit_present_SA[3])
```

```{r make NSA model}
# Non-Seasonally-Adjusted
co2_fit_present_NSA <- co2_present_NSA_train %>% 
  fill_gaps() %>%
  model(
    random_walk = ARIMA(average ~ 0 + pdq(0,1,0) + PDQ(0,0,0)),
    arima_fit = ARIMA(average ~ 0 + pdq(0:10,1,0:10) + PDQ(0:10,0,0:10)),
    arima_fit_log = ARIMA(log(average) ~ 0 + pdq(0,1,0) + PDQ(0:10,0,0:10))
  )
report(co2_fit_present_NSA[1])
report(co2_fit_present_NSA[2])
report(co2_fit_present_NSA[3])
```

Using the information criteria of AICc, we see that the best SA model was an ARIMA(5,1,0)(1,0,0)[52] with a log transformation. This model has five AR terms and is first differenced. It also has one seasonal AR term with a period of 52 weeks. The best NSA model was an ARIMA(0,1,0)(0,0,1)[52] with a log tranformation. This model is first differenced and has one seasonal MA term where the period is 52 weeks. We selected both of these models because they had the lowest AICc. We will examine the residuals to see if they resemble white noise.

```{r SA residals}
co2_fit_present_SA %>% 
  select(arima_fit) %>%
  gg_tsresiduals()

augment(co2_fit_present_SA) %>%
  # filter(.model == "arima_fit") %>% 
  features(.innov, ljung_box, lag = 10, dof = 0)
```

```{r NSA residals}
co2_fit_present_NSA %>% 
  select(arima_fit) %>%
  gg_tsresiduals()

augment(co2_fit_present_NSA) %>%
  # filter(.model == "arima_fit") %>% 
  features(.innov, ljung_box, lag = 10, dof = 0)
```

The top performing models for SA and NSA data both has residuals that rejected the null hypothesis of the Ljung-Box test, which indicates that they do not have white noise residuals. Therefore, we selected the models with the second lowest AICc, which have residuals that follow a normal distribution, and appear to be white noise in their ACF plots. Additionally, they both fail to reject the null hypothesis of the Ljung-Box test, indicating that the residuals exhibit no autocorrelation for 10 lags and can be regarded as white noise (SA p-value = 0.23, NSA p-value = 0.48).

The superior model for the SA data is an ARIMA(5,1,0)(1,0,0)[52], which has five AR terms, first differencing, and one seasonal AR term with a period of 52 weeks. The superior model for the NSA data is an ARIMA(1,1,4)(0,0,1)[52], which has one AR term, first differencing, four MA terms, and one seasonal MA term with a period of 52.

```{r forecast SA}
co2_forecast_SA <- co2_fit_present_SA %>% 
  forecast(co2_present_SA_test)

co2_present2 <- co2_present %>%
  mutate(average = difference(average, 52))

co2_forecast_SA %>%
  autoplot(co2_present2)+
  labs(y = "SA Avg CO2 (ppm)",title = "Seasonally Adjusted Average CO2 ppm") + guides(colour = guide_legend(title = "Forecast"))
```

```{r forecast NSA}
co2_forecast_NSA <- co2_fit_present_NSA %>% 
  forecast(co2_present_NSA_test)

co2_forecast_NSA %>%
  autoplot(co2_present)+
  labs(y = "Avg CO2 (ppm)",title = "Average CO2 ppm (not seasonally adjusted)") + guides(colour = guide_legend(title = "Forecast"))
```

```{r}
accuracy(co2_forecast_NSA, co2_present_NSA_test)
accuracy(co2_forecast_SA, co2_present_SA_test)
```

-   STILL NEED TO DO THE FOLLOWING:\*
-   Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample,
-   Comparing candidate models and explaining your choice.
-   Fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.

```{r}
co2_present$average <- na.interp(co2_present$average)
dcmp <- co2_present %>% 
  model(stl = STL(average))

components(dcmp) %>%
  as_tsibble() %>%
  autoplot(average, colour="gray") +
  geom_line(aes(y=trend), colour = "#D55E00") +
  labs(y = "Average CO2 Levels (ppm)", x="Time",
    title = "Average CO2 Levels with seasonally adjusted trend")

components(dcmp)
```

<!-- ## (3 points) Task Part 6b: How bad could it get? -->

<!-- With the non-seasonally adjusted data series, generate predictions for when atmospheric CO2 is expected to be at 420 ppm and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric CO2 levels in the year 2122. How confident are you that these will be accurate predictions? -->

# Conclusion