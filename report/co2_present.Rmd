---
title: "Present Global $CO_{2}$ Emissions"
short: "What Keeling missed all these years"
author: "Meral Basit, Alex Hubbard, Mohamed Bakr"
output:
  bookdown::pdf_document2:
    toc: true
    # citation_package: natbib
    fig_caption: true
    latex_engine: pdflatex
fontsize: 11pt
geometry: margin=1in
editor_options: 
  chunk_output_type: inline
bibliography: BibFile.bib
csl: data-science-journal.csl
---

```{r setup, echo=FALSE}
## default to not show code, unless we ask for it.
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE, dpi=1000)
options(digits = 3)
```

```{r load packages, echo = FALSE, message = FALSE}
library(tidyverse)
library(tsibble)
library(ggtext)
library(kableExtra)

library(forecast)
library(patchwork)
library(fable)
library(feasts)
```

```{r TWS theme, echo = FALSE}
# color palette
BERKELEY_BLUE = "#003262"
CALIFORNIA_GOLD = "#FDB515"
FOUNDERS_ROCK = "#3B7EA1"
MEDALIST = "#C4820E"

custom_palette <- c(
"WHITE" = "#FFFFFF",
"BERKELEY_BLUE" = BERKELEY_BLUE,
"CALIFORNIA_GOLD" = CALIFORNIA_GOLD,
"BLACK" = "#000000",
"FOUNDERS_ROCK" = FOUNDERS_ROCK,
"MEDALIST" = MEDALIST
)

## Create bar chart theme (Adapting Storytelling with Data guidance)
theme_swd_bar <- theme(
  plot.title = element_markdown(size = rel(1.5), color = "#000000"),
  plot.subtitle = element_markdown(),
  text = element_text(color = "#979797"),
  axis.title.x = element_text(hjust = 0, vjust = -0.5),
  axis.title.y = element_text(hjust = 1),
  line = element_blank(),
  rect = element_blank(),
  legend.position = "none",
  legend.justification='left'
)

# Properly format the commas while also removing the decimal places.
scaleFUN <- function(x) format(round(as.numeric(x), 0), nsmall=0, big.mark=",")
```

\newpage

<!-- # Report from the Point of View of the Present -->

<!-- One of the very interesting features of Keeling and colleagues' research is that they were able to evaluate, and re-evaluate the data as new series of measurements were released. This permitted the evaluation of previous models' performance and a much more difficult question: If their models' predictions were "off" was this the result of a failure of the model, or a change in the system? -->

# Introduction
<!-- ## (1 point) Task 0b: Introduction -->

<!-- In this introduction, you can assume that your reader will have **just** read your 1997 report. In this introduction, **very** briefly pose the question that you are evaluating, and describe what (if anything) has changed in the data generating process between 1997 and the present. -->

## Background

In the 1997 report, we explored the trend and variability in atmospheric $CO_2$ levels using polynomial and ARIMA models. Our analysis indicated a significant upward trend in $CO_2$ levels, with an accelerating rate of increase. In this report, we aim to re-evaluate those models and their predictions using the $CO_2$ level data realized after 1997. The central question we are addressing is:

```{=tex}
\begin{quote}
  \textit{Have the previous models accurately predicted the realized atmospheric $CO_2$ levels?}
\end{quote}
```

## Null Hypothesis

The predicted atmospheric $CO_2$ levels from the estimated models are equal to the realized atmospheric $CO_2$ levels for the period from January 1998 to June 2024. 
$$H_0: \hat{y}_t - y_t= 0$$
Where: $\hat{y_t}$ is the forecasted atmospheric $CO_2$ levels using the polynomial and ARIMA models and ${y_t}$ is the realized atmospheric $CO_2$ levels at time $t$ .

# Measurement and Data
<!-- The most current data is provided by the United States' National Oceanic and Atmospheric Administration, on a data page [[here](https://gml.noaa.gov/ccgg/trends/data.html)]. Gather the most recent weekly data from this page. (A group that is interested in even more data management might choose to work with the [hourly data](https://gml.noaa.gov/aftp/data/trace_gases/co2/in-situ/surface/mlo/co2_mlo_surface-insitu_1_ccgg_HourlyData.txt).) -->

<!-- Create a data pipeline that starts by reading from the appropriate URL, and ends by saving an object called `co2_present` that is a suitable time series object. -->

<!-- Conduct the same EDA on this data. Describe how the Keeling Curve evolved from 1997 to the present, noting where the series seems to be following similar trends to the series that you "evaluated in 1997" and where the series seems to be following different trends. This EDA can use the same, or very similar tools and views as you provided in your 1997 report. -->

## Measuring Atmospheric Carbon

Up until April 2019, measurements were collected using infrared absorption. After April 2019, a new CO2 analyzer was installed which uses Cavity Ring-Down Spectroscopy (CRDS). This change in devices could impact the errors in our predictions. Additionally, since our last report, the volcano near the research center has erupted. Therefore, the measurements from Dec. 2022 to July 4, 2023 are from the Maunakea Observatories, which are just over 20 miles north of the original observatory. There is also a note that the last several months worth of data is "preliminary" and therefore could be revised.

```{r read data}
co2_p <- read_csv("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv", skip = 35)

n_before_1997 <- co2_p |> filter(year <= 1997) |> nrow()
n_jul_2024 <-  co2_p |> filter(year == 2024 & month == 7) |> nrow()
n_na <- co2_p |> filter(average < 0 & year > 1997) |> nrow()

n_1 <- (nrow(co2_p) - n_before_1997)
n_2 <- (nrow(co2_p) - n_before_1997 - n_jul_2024)
n_3 <- (nrow(co2_p) - n_before_1997 - n_jul_2024 - n_na)

# remove values less than 0 and group by month
co2_present <- co2_p %>% 
  filter(average > 0) %>% 
  mutate(
    date_time = make_datetime(year, month, day),
    index = yearmonth(date_time)
  ) %>%
  as_tsibble(index = index, key=day) %>% 
  fill_gaps()%>% 
  
  index_by(index) %>%
  summarise(average = mean(average, na.rm = TRUE)) %>% 
  filter(index >= yearmonth("1998 Jan"))

# Checking Duplicate Data
sum(duplicated(co2_present))
# Checking Missing data and null data
colSums(is.na(co2_present) | co2_present$average < 0)
```

```{r make summary table}
# Create the data frame
accounting_table <- data.frame(
  Cause = c("Start", "Before 1998", "July 2024", "Values = -1000 (*missing*)"),
  Available = c(nrow(co2_p), n_1, n_2, n_3),
  Remove = c(NA, n_before_1997, n_jul_2024, n_na)
)
```

```{r Accounting-table, include=TRUE, fig.pos='H'}
kable(
  accounting_table,
  col.names = c("Cause", "Number of Samples Available For Analysis (after removal for cause)", "Number of Samples Removed"),
  digits = 3,
  caption = "Accounting Table", 
  booktabs = TRUE,
) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(1, width = "15em") %>% 
  column_spec(2, width = "15em") %>%
  column_spec(3, width = "10em")
```

The data was obtained from the United States' National Oceanic and Atmospheric Administration $NOAA$ [[accessible here](https://gml.noaa.gov/ccgg/trends/data.html)]. The dataset contains 2,617 observations of weekly atmospheric concentrations of $CO_2$ levels, span from the 3rd week of May 1974 to the 1st week of July 2024. As detailed in Table \@ref(tab:Accounting-table), we excluded `r n_before_1997` observations before 1998, `r n_jul_2024` observation for one week in July 2024 as the month data was incomplete, and `r n_na` observations where average $CO_2$ = -1000, which indicates missing readings. We ended up with 1,379 observations after cleaning up the data. Then, we calculated the monthly averages from January 1998 to the Jun 2024 to address the 4 missing weekly values and to facilitate comparison with the forecast data from the 1997 report, which was also in by month.

```{r EDA standard plots for present data}
# time series
p8 <- ggplot(co2_present, aes(x = index, y = average)) +
  geom_line(linewidth = .5) +
  labs(title = "Atmospheric CO2 1974-2024", x = "Date", y = "CO2 (ppm)") +
  theme_minimal()

# acf
p9 <- ggAcf(co2_present$average) + ggtitle("ACF CO2")

# pacf
p10 <- ggPacf(co2_present$average) + ggtitle("PACF CO2")

# histogram
p11 <- ggplot(co2_present, aes(x = average)) +
  geom_histogram(bins = 40, fill = "lightblue", color = "black") +
  labs(title = "Histogram of Atmospheric CO2", x = "CO2 (ppm)", y = "Frequency")

# annual average
p12 <- as_tibble(co2_present) %>%
  mutate(year = year(index)) %>%
  group_by(year) %>%
  summarise(avg_value = mean(average, na.rm=TRUE)) %>%
  ggplot(aes(x=year, y=avg_value)) +
  geom_point() +
  labs(title = "Average Yearly CO2 Levels", x = "Year", y = "Average CO2 (ppm)") +
  theme_minimal()

# make layout
pw <- (p8 + p11) /
  (p9 + p10) /
  p12

```

```{r EDA-plots, include=TRUE,fig.cap="Atmospheric CO2 EDA standard plots", fig.width=9, fig.height=5}
pw + plot_annotation(
  title = 'Atmospheric CO2 EDA plots 1998 - 2024',
  subtitle = 'Upward trend with clear seasonal pattern',
  caption = "Source: The United States' National Oceanic and Atmospheric Administration (NOAA)"
)
```

## Historical vs Present Trends in Atmospheric Carbon

We see that the atmospheric $CO_2$ levels continued to have a strong upward linear trend since 1997, as seen in the bottom plot of average yearly $CO_2$ in Figure \@ref(fig:EDA-plots). It appears that this linear trend very slightly increased in slope after the year 2000. We also see that the distribution of values is fairly wide from the histogram, with a slight right skew. We also see that the ACF tails off very slowly while the PACF drops off after lag 1 but still has few lags above the significance level. This indicates that there may be some unit roots. As this timeseries is a continuation of our previous time series, we know that this data is also not stationary.

# Old Models Comparisons

## Linear Model
<!-- ## (1 point) Task 2b: Compare linear model forecasts against realized CO2 -->

<!-- Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from a linear time model in 1997 (i.e. "Task 2a"). (You do not need to run any formal tests for this task.) -->

Our polynomial model did a pretty good job at predicting $CO_2$ up to 2024. It is missing the peaks and valleys, but it looks to capture the average yearly increase in $CO_2$ levels as shown in Figure \@ref(fig:poly-vs-arima-plot).

```{r linear-compare}
co2_poly_preditions <- read_csv("./data/co2_poly_preditions.csv") |> 
  mutate(index = yearmonth(index))

co2_poly_preditions <- co2_poly_preditions |> as_tsibble()

lm.plot <- ggplot(co2_present, aes(x = index)) +
  geom_line(aes(y = average)) +
  geom_line(data = co2_poly_preditions, aes(x = index, y = predictions), color = FOUNDERS_ROCK) +
  labs(title = paste("Forecast with <strong><span style='color:" , FOUNDERS_ROCK ,"'>Polynomial</span></strong> Model"), x = "Month", y = "CO2 (ppm)") +
  theme_swd_bar
```

## ARIMA Model
<!-- ## (1 point) Task 3b: Compare ARIMA models forecasts against realized CO2 -->

<!-- Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from the ARIMA model that you fitted in 1997 (i.e. "Task 3a"). Describe how the Keeling Curve evolved from 1997 to the present. -->

In the 1997 report, we used an ARIMA(3,1,0)(2,1,0)[12] model to forecast atmospheric $CO_2$ levels. Both the ARIMA model forecast and the the realized $CO_2$ show an upward trend with seasonal pattern. However, the model predicted lower values than the realized values in the long term as shown in Figure \@ref(fig:poly-vs-arima-plot). 

```{r ARIMA-compare}
co2.Arima.predictions <- read_csv("./data/co2_arima_preditions.csv") |> 
  mutate(index = yearmonth(index))

co2.Arima.predictions <- co2.Arima.predictions |> as_tsibble()

arima.plot <- ggplot(co2_present, aes(x = index)) +
  geom_line(aes(y = average)) +
  geom_line(data = co2.Arima.predictions, aes(x = index, y = value), color = CALIFORNIA_GOLD) +
  labs(title = paste("Forecast with <strong><span style='color:" , CALIFORNIA_GOLD ,"'>ARIMA</span></strong> Model"), x = "Month", y = "CO2 (ppm)") +
  theme_swd_bar
```

```{r poly-vs-arima-plot, include=TRUE,fig.cap="Comparing Realized Atmospheric CO2 Levels with Polynomial and ARIMA Models", fig.width=9, fig.height=3}
lm.plot + arima.plot + plot_annotation(
  title = 'Reliazed vs Forecasted Atmospheric CO2 plots 1998 - 2024',
  subtitle = 'The polynomial model performs better than the ARIMA model, when compared to actual data'
)
```

## Linear vs ARIMA

<!-- ## (3 points) Task 4b: Evaluate the performance of 1997 linear and ARIMA models -->

<!-- In 1997 you made predictions about the first time that CO2 would cross 420 ppm. How close were your models to the truth? -->

```{r predictions of 420 ppm}

# Find the first time CO2 levels reach 420 ppm in lm forecast
time_420.lm <- co2_poly_preditions |> filter(predictions >= 420) |> slice(1)
date_420.lm <- time_420.lm$index

# Find the first time CO2 levels reach 420 ppm in ARIMA forecast
time_420.arima <- co2.Arima.predictions |> filter(value >= 420) |> slice(1)
date_420.arima <- time_420.arima$index

# Find the first time CO2 levels reach 420 ppm in actual
time_420_actual <- co2_present |> filter(average >= 420) |> slice(1)
date_420_actual <- time_420_actual$index
```

In 1997, we predicted that $CO_2$ levels would reach 420 PPM by 2025 March using our ARIMA model and `r date_420.lm` using the Polynomial model. However, actual $CO_2$ levels reached 420 PPM by `r date_420_actual`. Our polynomial model was much closer to the true outcome in this case.

<!-- After reflecting on your performance on this threshold-prediction task, continue to use the weekly data to generate a month-average series from 1997 to the present, and compare the overall forecasting performance of your models from Parts 2a and 3b over the entire period. (You should conduct formal tests for this task.) -->

```{r month-average series from 1997 to present}
# co2_present <- co2_present %>% 
#   mutate(
#     index = yearmonth(year_week)
#   )
# 
# monthly_avg <- co2_present %>%
#   index_by(index) %>%
#   summarise(value = mean(average, na.rm = TRUE)) %>% 
#   filter(index >= ymd(19950101))
# 
# forecast_present <- co2_fit %>% 
#   forecast(monthly_avg)
# 
# forecast_present %>% 
#   accuracy(monthly_avg)

rmse_lm <- sqrt(mean((co2_present$average - co2_poly_preditions$predictions)^2))
rmse_arima <- sqrt(mean((co2_present$average - co2.Arima.predictions$value)^2))
```

Our ARIMA model with log transformation produces RMSE = `r rmse_arima`, while the Polynomial model produces an RMSE = `r rmse_lm`. Considering the descriptive analysis, threshold-prediction results, and the RMSE comparisons, we can conclude that despite the fact that the ARIMA model performed better with the 1997 data, the Polynomial model outperformed the ARIMA model in the long-term forecast. 

# New Model

## ARIMA Model

<!-- ## (4 points) Task 5b: Train best models on present data -->

<!-- Seasonally adjust the weekly NOAA data, and split both seasonally-adjusted (SA) and non-seasonally-adjusted (NSA) series into training and test sets, using the last two years of observations as the test sets. For both SA and NSA series, fit ARIMA models using all appropriate steps. Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample, comparing candidate models and explaining your choice. In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model. -->

We made two copies of the `NOAA` data and seasonally adjusted one of them. After that, we split both datasets (the seasonally adjusted $(SA)$ and the non-seasonally adjusted $(NSA)$) into training and test sets, using the last two years of observations as the test sets.
```{r split new data}

# Seasonally adjust the data
co2_sa <- co2_present |> 
  model(STL(average ~ season(window = "periodic"))) |> 
  components() |>
  mutate(average = season_adjust) |> 
  select(index, average)

# Split the data into training and test sets
co2_present_SA_train <- filter(co2_sa, index < yearmonth("2022-01"))
co2_present_SA_test <- filter(co2_sa, index >= yearmonth("2022-01"))

co2_present_NSA_train <- filter(co2_present, index < yearmonth("2022-01"))
co2_present_NSA_test <- filter(co2_present, index >= yearmonth("2022-01"))
```

```{r new data stationary}
# SA Data
co2_present_SA_train %>% 
  features(average, unitroot_kpss)

co2_present_SA_train %>% 
  mutate(diff = difference(average)) %>% 
  features(diff, unitroot_kpss)

# NSA Data
co2_present_NSA_train %>% 
  features(average, unitroot_kpss)

co2_present_NSA_train %>% 
  mutate(diff = difference(average)) %>% 
  features(diff, unitroot_kpss)
  
```

We used the KPSS test to determine whether the SA and the NSA data are stationary. For both series, the first test yields a p-value of 0.01, leading us to reject the null hypothesis, indicating that the data is not stationary. After taking one difference, the p-value for both series rose to 0.1, and we failed to reject the null hypothesis, suggesting that both datasets are stationary after one difference.

```{r make SA model}
# Seasonally-Adjusted
co2_fit_present_SA <- co2_present_SA_train %>% 
  fill_gaps() %>%
  model(
    arima_fit.search = ARIMA(average, stepwise = FALSE),
    arima_fit_log.search = ARIMA(log(average), stepwise = FALSE)
  )

# Estimated models results
co2_fit_present_SA |> pivot_longer(cols = everything(), names_to = "Model name",
                         values_to = "Orders") |> 
  knitr::kable(caption = "Estimated ARIMA Models")
```
```{r sa-arima-results, include=TRUE, fig.cap="ARIMA Models Comparison - Seasonally Adjusted CO2 values"}
# Model Comparison
glance(co2_fit_present_SA) |>
  arrange(AICc) |> 
  select(.model:BIC) |> 
  mutate_if(is.numeric, round, digits =2) |> 
  knitr::kable(caption = "ARIMA Models Comparison - Seasonally Adjusted CO2 values")
```


```{r make NSA model}
# Non-Seasonally-Adjusted
co2_fit_present_NSA <- co2_present_NSA_train %>% 
  fill_gaps() %>%
  model(
    arima_fit.search = ARIMA(average, stepwise = FALSE),
    arima_fit_log.search = ARIMA(log(average), stepwise = FALSE)
  )

# Estimated models results
co2_fit_present_NSA |> pivot_longer(cols = everything(), names_to = "Model name",
                         values_to = "Orders") |> 
  knitr::kable(caption = "Estimated ARIMA Models")
```

```{r nsa-arima-results, include=TRUE, fig.cap="ARIMA Models Comparison - non-Seasonally Adjusted CO2 values"}
# Model Comparison
glance(co2_fit_present_NSA) |>
  arrange(AICc) |> 
  select(.model:BIC) |> 
  mutate_if(is.numeric, round, digits =2) |> 
  knitr::kable(caption = "ARIMA Models Comparison - non-Seasonally Adjusted CO2 values")
```

Based on the EDA performed earlier, it was challenging to estimate the $p$ and $q$ terms for the ARIMA model just by looking at the `ACF` and `PACF` plots. We estimated two non-stepwise ARIMA models for each set, one with log transformation and another without the log transformation. Using the information criteria `AICc` in Table \@ref(tab:sa-arima-results), we see that the best $SA$ model was an ARIMA(5,1,0)(1,0,0)[52] with a log transformation. This model has five `AR` terms and is first differenced. It also has one seasonal `AR` term with a period of 52 weeks. The best $NSA$ model was an ARIMA(0,1,0)(0,0,1)[52] with a log tranformation. This model is first differenced and has one seasonal `MA` term where the period is 52 weeks. We selected both of these models because they had the lowest `AICc`. We will examine the residuals to see if they resemble white noise.

```{r SA-residals, include=TRUE, fig.cap="SA Trained ARIMA(5,1,0)(1,0,0)[52] Model Residuals", fig.width=9, fig.height=3}
co2_fit_present_SA %>% 
  select(arima_fit_log.search) %>%
  gg_tsresiduals()
```
```{r}
augment(co2_fit_present_SA) %>%
  # filter(.model == "arima_fit") %>% 
  features(.innov, ljung_box, lag = 10, dof = 0)
```


```{r NSA-residals, include=TRUE, fig.cap="NSA Trained ARIMA(0,1,0)(0,0,1)[52] Model Residuals", fig.width=9, fig.height=3}
co2_fit_present_NSA %>% 
  select(arima_fit_log.search) %>%
  gg_tsresiduals()
```
```{r}
augment(co2_fit_present_NSA) %>%
  # filter(.model == "arima_fit") %>% 
  features(.innov, ljung_box, lag = 10, dof = 0)
```

The top performing models for $SA$ and $NSA$ data both have residuals that rejected the null hypothesis of the Ljung-Box test, which indicates that they do not have white noise residuals. Therefore, we selected the models with the second lowest `AICc`, which have residuals that follow a normal distribution, and appear to be white noise in their `ACF` plots. Additionally, they both fail to reject the null hypothesis of the Ljung-Box test, indicating that the residuals exhibit no autocorrelation for 10 lags and can be regarded as white noise ($SA$ p-value = 0.23, $NSA$ p-value = 0.48).

The superior model for the $SA$ data is an ARIMA(5,1,0)(1,0,0)[52], which has five `AR` terms, first differencing, and one seasonal `AR` term with a period of 52 weeks. The superior model for the $NSA$ data is an ARIMA(1,1,4)(0,0,1)[52], which has one `AR` term, first differencing, four `MA` terms, and one seasonal `MA` term with a period of 52 weeks.

```{r forecast SA}
co2_forecast_SA <- co2_fit_present_SA |> 
  forecast(co2_present_SA_test)

# co2_present2 <- co2_present %>%
#   mutate(average = difference(average, 12))

sa.plot <- co2_forecast_SA |> 
  autoplot(colour="cornflowerblue") + 
  autolayer(co2_present, colour="black") + 
  geom_line(data=co2_fit_present_SA %>% augment(), aes(index,.fitted,color=.model)) +
  facet_wrap(~.model, ncol=1, nrow=5) +
  labs(title = "Seasonally Adjusted", x = "Month", y = "SA Avg CO2 (ppm)") +
  theme_swd_bar
```

```{r forecast NSA}
co2_forecast_NSA <- co2_fit_present_NSA |> 
  forecast(co2_present_NSA_test)

nsa.plot <- co2_forecast_NSA |> 
  autoplot(colour="cornflowerblue") + 
  autolayer(co2_present, colour="black") + 
  geom_line(data=co2_fit_present_NSA |> augment(), aes(index,.fitted,color=.model)) +
  facet_wrap(~.model, ncol=1, nrow=5)+
  labs(title = "non-Seasonally Adjusted", x = "Month", y = "Avg CO2 (ppm)",) +
  theme_swd_bar
```
```{r sa-vs-nsa-plot, include=TRUE, fig.cap="Comparing SA vs NSA data sets ARIMA models perfomance", fig.width=9, fig.height=3}
sa.plot + nsa.plot + plot_annotation(
  title = 'SA vs NSA Atmospheric CO2 ARIMA Models',
  subtitle = "The NSA trained model outperforms the SA trained. However SA successfully captures the trend.")
```

```{r rmse-sa-nsa}
rmse.sa <- accuracy(co2_forecast_SA, co2_present_SA_test)['RMSE'] |> arrange(RMSE) |> slice(1)
rmse.nsa <- accuracy(co2_forecast_NSA, co2_present_NSA_test)['RMSE'] |> arrange(RMSE) |> slice(1)
```

The selected $NSA$ trained model produced RMSE = `r round(rmse.nsa, 2)` which outperforms the selected $SA$ trained model that produced RMSE = `r round(rmse.sa, 2)` 

## Polynomial Model

We estimated a Polynomial model of the form: $CO2_t= \beta_0 + \beta_1 t + \beta_2 t^2 + \epsilon_t$ and trained it on the $SA$ data. Based on the model fit results, the estimated coefficient $\beta_1 = 0.14$ indicates that the $CO_2$ levels increase by $\approx0.14$ units per month which is lower than the rate of $\approx0.0674$ we estimated in the 1997 report. The p-value of the time index is $<0.05$ which suggests that the coefficient is statistically significant. We reject the null hypothesis that the coefficient $\beta_1 = 0$. This also provides evidence that the $CO_2$ levels continue to have an upward linear trend. The estimated quadratic term coefficient $\beta_2 = 0.000141$. The positive coefficient suggests that the rate of increase in $CO_2$ levels is accelerating at a higher rate than the model estimated in 1997 report. The p-value of the time index is $<0.05$ which suggests that the coefficient is statistically significant. We reject the null hypothesis that the coefficient $\beta_2 = 0$.

```{r fit polynomial model}
# Add seasonal dummy variables
co2_present_SA_train <- co2_present_SA_train |> 
  mutate(month = factor(month(index)))

# Fit the polynomial model with seasonal dummies
mod.poly.season2 <- lm(average ~ time(index) + I(time(index)^2), data = co2_present_SA_train)
summary(mod.poly.season2)

# Plot the polynomial model

poly.mod.plot.insample <- co2_present_SA_train |>
ggplot(aes(x = index)) +
  geom_line(aes(y = average)) +
  geom_line(aes(y = predict(mod.poly.season2, newdata = as_tibble(co2_present_SA_train))), color = BERKELEY_BLUE) + #ran out of time but this isn't the same length as year_week so it's causing issues
  labs(title = paste("<strong><span style='color:" , BERKELEY_BLUE ,"'>In-Sample</span></strong> Forecast"), x = "Month", y = "CO2 (ppm)") +
  theme_swd_bar
```

```{r}
co2_present_SA_test <- co2_present_SA_test |> 
  mutate(month = factor(month(index)))

# Plot the polynomial model

poly.mod.plot.psuedo <- co2_present_SA_test |>
ggplot(aes(x = index)) +
  geom_line(aes(y = average)) +
  geom_line(aes(y = predict(mod.poly.season2, newdata = as_tibble(co2_present_SA_test))), color = MEDALIST) + #ran out of time but this isn't the same length as year_week so it's causing issues
  labs(title =  paste("<strong><span style='color:" , MEDALIST ,"'>Out-of-Sample</span></strong> Forecast"), x = "Month", y = "CO2 (ppm)") + 
  theme_swd_bar
```

```{r sa-poly-plot, include=TRUE, fig.cap="SA trained Ploynomial Model - in-Sample vs Psuedo", fig.width=9, fig.height=3}
poly.mod.plot.insample + poly.mod.plot.psuedo + plot_annotation(
  title = 'Polynomial Model in-Sample vs Psuedo Atmospheric CO2 Forecasted based',
  subtitle = "In-sample forecast had a better results than the psuedo forecast.")
```

```{r}
rmse.sa.ploy.out <- accuracy(predict(mod.poly.season2, newdata = as_tibble(co2_present_SA_test)), co2_present_SA_test$average)[, "RMSE"]
rmse.sa.poly.in <- accuracy(predict(mod.poly.season2, newdata = as_tibble(co2_present_SA_train)), co2_present_SA_train$average)[, "RMSE"]
```
Our polynomial model performs well in sample, but appears to underpredict out-of-sample. The polynomial model produced RMSE = `r round(rmse.sa.ploy.out, 2)` which is much higher than the ARIMA model out of sample RMSE = `r round(rmse.sa, 2)`.

```{r}
co2_present$average <- na.interp(co2_present$average)
dcmp <- co2_present %>% 
  model(stl = STL(average))

components(dcmp) %>%
  as_tsibble() %>%
  autoplot(average, colour="gray") +
  geom_line(aes(y=trend), colour = "#D55E00") +
  labs(y = "Average CO2 Levels (ppm)", x="Time",
    title = "Average CO2 Levels with seasonally adjusted trend")

components(dcmp)
```

<!-- ## (3 points) Task Part 6b: How bad could it get? -->

<!-- With the non-seasonally adjusted data series, generate predictions for when atmospheric CO2 is expected to be at 420 ppm and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric CO2 levels in the year 2122. How confident are you that these will be accurate predictions? -->

```{r}
co2_forecast_NSA_2122 <- co2_fit_present_NSA %>% 
  forecast(h = "100 years") %>% 
  filter(.model == "arima_fit_log.search")

pred_2121 <- co2_forecast_NSA_2122 %>% 
  filter(index == yearmonth("21211201"))

pred_500ppm <- co2_forecast_NSA_2122 %>% 
  filter(.mean >= 500)
```

In 1997, we predicted that CO2 levels would reach 500 ppm by the year 2050. Our updated prediction is that we will reach 500 ppm by `r round(pred_500ppm$index[1], 2)`. This indicates that the updated data may support a slowing of the growth in atmospheric CO2. By 2122, our model predicts that CO2 levels will reach `r round(pred_2121$.mean, 2)` ppm. As with the 1997 report, we have low confidence in these predictions, as we are well beyond the range of our data, and have no way of accounting for improvements in efficiency, grid electrification, etc.

# Conclusion

The updated atmospheric CO2 data shows that the increasing trend in CO2 levels continued roughly as expected. We still see significant coefficients in our predictive models, and forecast that CO2 levels will continue to rise into the future, barring any significant intervention. As follows, we again reject our null hypothesis.
